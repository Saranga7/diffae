{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CelebHQAttrDataset, AnnotatedFFHQDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/pr_deepdevpath/Saranga/diffae/dataset.py:623: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  self.df = pd.read_csv(f, delim_whitespace=True)\n"
     ]
    }
   ],
   "source": [
    "data = CelebHQAttrDataset(image_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[-0.7804, -0.7725, -0.7569,  ...,  0.2392,  0.2392,  0.2392],\n",
       "          [-0.8039, -0.7882, -0.7882,  ...,  0.2392,  0.2392,  0.2392],\n",
       "          [-0.8275, -0.8118, -0.8039,  ...,  0.2471,  0.2392,  0.2392],\n",
       "          ...,\n",
       "          [-0.1216, -0.1137, -0.1216,  ...,  0.0039,  0.0039,  0.0039],\n",
       "          [-0.1294, -0.1294, -0.1216,  ...,  0.0039,  0.0039,  0.0039],\n",
       "          [-0.1451, -0.1451, -0.1373,  ...,  0.0039,  0.0039,  0.0039]],\n",
       " \n",
       "         [[-0.8588, -0.8510, -0.8588,  ..., -0.6392, -0.6392, -0.6392],\n",
       "          [-0.8902, -0.8745, -0.8745,  ..., -0.6392, -0.6392, -0.6392],\n",
       "          [-0.8980, -0.8824, -0.8745,  ..., -0.6314, -0.6392, -0.6392],\n",
       "          ...,\n",
       "          [-0.3804, -0.3725, -0.3804,  ...,  0.0039,  0.0039,  0.0039],\n",
       "          [-0.3725, -0.3725, -0.3804,  ...,  0.0039,  0.0039,  0.0039],\n",
       "          [-0.3882, -0.3882, -0.3961,  ...,  0.0039,  0.0039,  0.0039]],\n",
       " \n",
       "         [[-0.9294, -0.9216, -0.9059,  ..., -0.7255, -0.7255, -0.7255],\n",
       "          [-0.9373, -0.9216, -0.9216,  ..., -0.7255, -0.7255, -0.7255],\n",
       "          [-0.9373, -0.9216, -0.9137,  ..., -0.7176, -0.7255, -0.7255],\n",
       "          ...,\n",
       "          [-0.5294, -0.5216, -0.5294,  ...,  0.0039,  0.0039,  0.0039],\n",
       "          [-0.5294, -0.5294, -0.5294,  ...,  0.0039,  0.0039,  0.0039],\n",
       "          [-0.5451, -0.5451, -0.5451,  ...,  0.0039,  0.0039,  0.0039]]]),\n",
       " 'index': 56,\n",
       " 'labels': tensor([-1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "          1,  1, -1,  1, -1, -1,  1, -1, -1,  1,  1,  1, -1,  1, -1,  1,  1, -1,\n",
       "          1, -1, -1,  1])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[56]['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 0., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data[56]['labels']\n",
    "gt = torch.where(labels > 0,\n",
    "                             torch.ones_like(labels).float(),\n",
    "                             torch.zeros_like(labels).float())\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = AnnotatedFFHQDataset('/projects/deepdevpath/Saranga/Explaining-In-Style-Reproducibility-Study/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[-0.9686, -1.0000, -0.9843,  ..., -0.9922, -1.0000, -1.0000],\n",
       "          [-0.9765, -1.0000, -0.9922,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-0.9686, -0.9922, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-0.8353, -0.8196, -0.8118,  ...,  0.0196,  0.0275,  0.0353],\n",
       "          [-0.8667, -0.8588, -0.8510,  ...,  0.0667,  0.0824,  0.0745],\n",
       "          [-0.8745, -0.8667, -0.8588,  ...,  0.0275,  0.0588,  0.0588]],\n",
       " \n",
       "         [[ 0.0275, -0.0196,  0.0118,  ..., -0.0667, -0.0902, -0.0824],\n",
       "          [ 0.0196, -0.0118,  0.0039,  ..., -0.0980, -0.1059, -0.0902],\n",
       "          [ 0.0275,  0.0039, -0.0039,  ..., -0.1059, -0.1216, -0.1137],\n",
       "          ...,\n",
       "          [ 0.2000,  0.2157,  0.2235,  ...,  0.2157,  0.2157,  0.2235],\n",
       "          [ 0.1608,  0.1686,  0.1765,  ...,  0.2549,  0.2471,  0.2392],\n",
       "          [ 0.1529,  0.1608,  0.1686,  ...,  0.2157,  0.2235,  0.2235]],\n",
       " \n",
       "         [[ 0.1451,  0.0980,  0.1451,  ...,  0.2157,  0.1843,  0.1922],\n",
       "          [ 0.1373,  0.1059,  0.1373,  ...,  0.1922,  0.1686,  0.1843],\n",
       "          [ 0.1608,  0.1373,  0.1294,  ...,  0.1843,  0.1686,  0.1765],\n",
       "          ...,\n",
       "          [ 0.3176,  0.3333,  0.3412,  ...,  0.2549,  0.2471,  0.2549],\n",
       "          [ 0.3020,  0.3098,  0.3176,  ...,  0.2863,  0.2863,  0.2784],\n",
       "          [ 0.2941,  0.3020,  0.3098,  ...,  0.2471,  0.2627,  0.2627]]]),\n",
       " 'index': 0,\n",
       " 'labels': tensor(0)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[ 0.2941,  0.2941,  0.3020,  ...,  0.5137,  0.5373,  0.4980],\n",
       "          [ 0.3333,  0.3176,  0.3020,  ...,  0.5137,  0.5294,  0.5137],\n",
       "          [ 0.3333,  0.3098,  0.3020,  ...,  0.5137,  0.5059,  0.5216],\n",
       "          ...,\n",
       "          [ 0.0588, -0.0980, -0.3725,  ...,  0.6235,  0.6000,  0.5922],\n",
       "          [-0.3176, -0.4039, -0.4667,  ...,  0.6157,  0.6000,  0.6000],\n",
       "          [-0.4588, -0.5137, -0.4510,  ...,  0.5843,  0.5608,  0.5843]],\n",
       " \n",
       "         [[ 0.0275,  0.0275,  0.0353,  ...,  0.6235,  0.6471,  0.6078],\n",
       "          [ 0.0667,  0.0510,  0.0353,  ...,  0.6235,  0.6392,  0.6235],\n",
       "          [ 0.0667,  0.0431,  0.0353,  ...,  0.6392,  0.6314,  0.6471],\n",
       "          ...,\n",
       "          [-0.0902, -0.2078, -0.4275,  ...,  0.5843,  0.5608,  0.5529],\n",
       "          [-0.3490, -0.4196, -0.4824,  ...,  0.5765,  0.5608,  0.5608],\n",
       "          [-0.4196, -0.4745, -0.4431,  ...,  0.5451,  0.5216,  0.5451]],\n",
       " \n",
       "         [[-0.1843, -0.1843, -0.1765,  ...,  0.6235,  0.6471,  0.6078],\n",
       "          [-0.1451, -0.1608, -0.1765,  ...,  0.6235,  0.6392,  0.6235],\n",
       "          [-0.1451, -0.1686, -0.1765,  ...,  0.6314,  0.6235,  0.6392],\n",
       "          ...,\n",
       "          [-0.1216, -0.2157, -0.3647,  ...,  0.6392,  0.6157,  0.6078],\n",
       "          [-0.2392, -0.3098, -0.3176,  ...,  0.6314,  0.6157,  0.6157],\n",
       "          [-0.2471, -0.3020, -0.2235,  ...,  0.6000,  0.5765,  0.6000]]]),\n",
       " 'index': 6392,\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[6392]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[6392]['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[6392]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Linear(512, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 512)\n",
    "pred = classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(data2, batch_size=64)\n",
    "\n",
    "batch = next(iter(dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.6785e-01],\n",
       "        [ 6.3465e-01],\n",
       "        [-1.1190e+00],\n",
       "        [-9.8396e-01],\n",
       "        [-3.1514e-01],\n",
       "        [-6.1823e-02],\n",
       "        [-6.8299e-01],\n",
       "        [ 3.0822e-01],\n",
       "        [ 6.6234e-01],\n",
       "        [ 2.8170e-01],\n",
       "        [-1.4998e+00],\n",
       "        [ 1.1529e-01],\n",
       "        [ 1.0124e+00],\n",
       "        [ 3.2166e-01],\n",
       "        [-1.3568e-01],\n",
       "        [-8.8271e-01],\n",
       "        [ 4.3810e-01],\n",
       "        [-4.0126e-02],\n",
       "        [-2.0673e-01],\n",
       "        [ 2.2897e-01],\n",
       "        [-1.0990e+00],\n",
       "        [-4.4274e-01],\n",
       "        [ 1.5857e-01],\n",
       "        [ 4.9157e-01],\n",
       "        [ 1.9862e-01],\n",
       "        [-6.3672e-02],\n",
       "        [-1.7471e-01],\n",
       "        [-5.9854e-01],\n",
       "        [ 1.3183e+00],\n",
       "        [ 6.9210e-01],\n",
       "        [ 3.1166e-01],\n",
       "        [ 1.4757e-03],\n",
       "        [ 1.9216e+00],\n",
       "        [ 1.1871e-01],\n",
       "        [ 3.9865e-01],\n",
       "        [-2.1237e-01],\n",
       "        [-3.4822e-01],\n",
       "        [ 3.5430e-01],\n",
       "        [-8.3735e-01],\n",
       "        [ 1.9773e-01],\n",
       "        [ 2.0905e-01],\n",
       "        [ 6.6396e-02],\n",
       "        [ 2.7827e-01],\n",
       "        [-8.0568e-01],\n",
       "        [-5.4353e-01],\n",
       "        [ 6.9878e-01],\n",
       "        [ 1.0555e-02],\n",
       "        [ 4.2171e-01],\n",
       "        [-4.9885e-01],\n",
       "        [ 2.4748e-01],\n",
       "        [ 2.7798e-02],\n",
       "        [ 3.2685e-01],\n",
       "        [ 1.0050e+00],\n",
       "        [-2.8452e-01],\n",
       "        [-2.6661e-02],\n",
       "        [-3.1141e-01],\n",
       "        [-3.1414e-01],\n",
       "        [ 2.5852e-01],\n",
       "        [ 3.9615e-02],\n",
       "        [ 1.9429e-01],\n",
       "        [-4.3070e-01],\n",
       "        [-9.8589e-02],\n",
       "        [ 5.3046e-01],\n",
       "        [ 7.8719e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "# gt = torch.randint(low=0, high=2, size=(64,))\n",
    "gt = batch['labels'].unsqueeze(1)\n",
    "print(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffae/lib/python3.12/site-packages/torch/nn/functional.py:3226\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(pred, gt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
