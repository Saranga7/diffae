/users/biocomp/mahanta/anaconda3/envs/diffae/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory checkpoints/ffhq128_autoenc_w_classifier_weightedLoss exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
/users/biocomp/mahanta/anaconda3/envs/diffae/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:432: UserWarning: ModelCheckpoint(save_last=True, save_top_k=None, monitor=None) is a redundant configuration. You can save the last checkpoint with ModelCheckpoint(save_top_k=None, monitor=None).
  rank_zero_warn(
Using native 16bit precision.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Global seed set to 0
Using cache found in /users/biocomp/mahanta/.cache/torch/hub/pytorch_vision_v0.10.0
Restoring states from the checkpoint file at checkpoints/ffhq128_autoenc_w_classifier_weightedLoss/last.ckpt
Global seed set to 0
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4
Using native 16bit precision.
Using native 16bit precision.
Using native 16bit precision.
Global seed set to 0
Global seed set to 0
Global seed set to 0
Using cache found in /users/biocomp/mahanta/.cache/torch/hub/pytorch_vision_v0.10.0
Using cache found in /users/biocomp/mahanta/.cache/torch/hub/pytorch_vision_v0.10.0
Using cache found in /users/biocomp/mahanta/.cache/torch/hub/pytorch_vision_v0.10.0
Global seed set to 0
Global seed set to 0
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4
Global seed set to 0
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 4 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Restored all states from the checkpoint file at checkpoints/ffhq128_autoenc_w_classifier_weightedLoss/last.ckpt

  | Name      | Type                 | Params
---------------------------------------------------
0 | model     | BeatGANsAutoencModel | 131 M 
1 | ema_model | BeatGANsAutoencModel | 131 M 
---------------------------------------------------
128 M     Trainable params
133 M     Non-trainable params
262 M     Total params
1,048.290 Total estimated model params size (MB)
/users/biocomp/mahanta/anaconda3/envs/diffae/lib/python3.12/site-packages/pytorch_lightning/callbacks/lr_monitor.py:112: RuntimeWarning: You are using `LearningRateMonitor` callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.
  rank_zero_warn(
conf: ffhq128_autoenc_w_classifier_weightedLoss
ckpt path: checkpoints/ffhq128_autoenc_w_classifier_weightedLoss/last.ckpt
resume!
Model params: 124.97 M
local seed: 2
train data: 70001
val data: 70001
on train dataloader start ...
conf: ffhq128_autoenc_w_classifier_weightedLoss
ckpt path: checkpoints/ffhq128_autoenc_w_classifier_weightedLoss/last.ckpt
resume!
Model params: 124.97 M
local seed: 3
train data: 70001
val data: 70001
on train dataloader start ...
conf: ffhq128_autoenc_w_classifier_weightedLoss
ckpt path: checkpoints/ffhq128_autoenc_w_classifier_weightedLoss/last.ckpt
resume!
Model params: 124.97 M
local seed: 1
train data: 70001
val data: 70001
on train dataloader start ...
conf: ffhq128_autoenc_w_classifier_weightedLoss
ckpt path: checkpoints/ffhq128_autoenc_w_classifier_weightedLoss/last.ckpt
resume!
Model params: 124.97 M
local seed: 0
train data: 70001
val data: 70001
on train dataloader start ...
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/546 [00:00<00:00, 27235.74it/s]Epoch 94:   0%|          | 0/546 [00:00<00:00, 5511.57it/s] Epoch 94:   0%|          | 1/546 [00:07<34:39,  3.82s/it]  Epoch 94:   0%|          | 1/546 [00:07<34:39,  3.82s/it, loss=0.0317, v_num=]Epoch 94:   0%|          | 2/546 [00:12<38:09,  4.21s/it, loss=0.0317, v_num=]Epoch 94:   0%|          | 2/546 [00:12<38:09,  4.21s/it, loss=0.0414, v_num=]Epoch 94:   1%|          | 3/546 [00:34<1:18:45,  8.70s/it, loss=0.0414, v_num=]Epoch 94:   1%|          | 3/546 [00:34<1:18:45,  8.70s/it, loss=0.0404, v_num=]Epoch 94:   1%|          | 4/546 [01:07<2:01:35, 13.46s/it, loss=0.0404, v_num=]Epoch 94:   1%|          | 4/546 [01:07<2:01:35, 13.46s/it, loss=0.0407, v_num=]Epoch 94:   1%|          | 5/546 [01:35<2:23:37, 15.93s/it, loss=0.0407, v_num=]Epoch 94:   1%|          | 5/546 [01:35<2:23:37, 15.93s/it, loss=0.0374, v_num=]Epoch 94:   1%|          | 6/546 [02:02<2:37:13, 17.47s/it, loss=0.0374, v_num=]Epoch 94:   1%|          | 6/546 [02:02<2:37:13, 17.47s/it, loss=0.0406, v_num=]Epoch 94:   1%|▏         | 7/546 [02:19<2:37:00, 17.48s/it, loss=0.0406, v_num=]Epoch 94:   1%|▏         | 7/546 [02:19<2:37:00, 17.48s/it, loss=0.0423, v_num=]Epoch 94:   1%|▏         | 8/546 [02:43<2:42:29, 18.12s/it, loss=0.0423, v_num=]Epoch 94:   1%|▏         | 8/546 [02:43<2:42:29, 18.12s/it, loss=0.041, v_num=] Epoch 94:   2%|▏         | 9/546 [03:09<2:49:57, 18.99s/it, loss=0.041, v_num=]Epoch 94:   2%|▏         | 9/546 [03:09<2:49:57, 18.99s/it, loss=0.0423, v_num=]Epoch 94:   2%|▏         | 10/546 [03:30<2:51:04, 19.15s/it, loss=0.0423, v_num=]Epoch 94:   2%|▏         | 10/546 [03:30<2:51:04, 19.15s/it, loss=0.0433, v_num=]Epoch 94:   2%|▏         | 11/546 [04:00<2:58:49, 20.06s/it, loss=0.0433, v_num=]Epoch 94:   2%|▏         | 11/546 [04:00<2:58:49, 20.06s/it, loss=0.0445, v_num=]Epoch 94:   2%|▏         | 12/546 [04:31<3:05:39, 20.86s/it, loss=0.0445, v_num=]Epoch 94:   2%|▏         | 12/546 [04:31<3:05:39, 20.86s/it, loss=0.0425, v_num=]Epoch 94:   2%|▏         | 13/546 [05:00<3:10:52, 21.49s/it, loss=0.0425, v_num=]Epoch 94:   2%|▏         | 13/546 [05:00<3:10:52, 21.49s/it, loss=0.0432, v_num=]Epoch 94:   3%|▎         | 14/546 [05:25<3:12:16, 21.69s/it, loss=0.0432, v_num=]Epoch 94:   3%|▎         | 14/546 [05:25<3:12:16, 21.69s/it, loss=0.0428, v_num=]Epoch 94:   3%|▎         | 15/546 [05:47<3:12:21, 21.74s/it, loss=0.0428, v_num=]Epoch 94:   3%|▎         | 15/546 [05:47<3:12:21, 21.74s/it, loss=0.0419, v_num=]Epoch 94:   3%|▎         | 16/546 [06:14<3:14:28, 22.02s/it, loss=0.0419, v_num=]Epoch 94:   3%|▎         | 16/546 [06:14<3:14:28, 22.02s/it, loss=0.0418, v_num=]Epoch 94:   3%|▎         | 17/546 [06:35<3:13:39, 21.96s/it, loss=0.0418, v_num=]Epoch 94:   3%|▎         | 17/546 [06:35<3:13:39, 21.96s/it, loss=0.0433, v_num=]Epoch 94:   3%|▎         | 18/546 [07:03<3:16:12, 22.30s/it, loss=0.0433, v_num=]Epoch 94:   3%|▎         | 18/546 [07:03<3:16:12, 22.30s/it, loss=0.0426, v_num=]Epoch 94:   3%|▎         | 19/546 [07:28<3:17:08, 22.44s/it, loss=0.0426, v_num=]Epoch 94:   3%|▎         | 19/546 [07:28<3:17:08, 22.44s/it, loss=0.0432, v_num=]Epoch 94:   4%|▎         | 20/546 [07:56<3:18:58, 22.70s/it, loss=0.0432, v_num=]Epoch 94:   4%|▎         | 20/546 [07:56<3:18:58, 22.70s/it, loss=0.0426, v_num=]Epoch 94:   4%|▍         | 21/546 [08:18<3:18:11, 22.65s/it, loss=0.0426, v_num=]Epoch 94:   4%|▍         | 21/546 [08:18<3:18:11, 22.65s/it, loss=0.0432, v_num=]Epoch 94:   4%|▍         | 22/546 [08:39<3:17:11, 22.58s/it, loss=0.0432, v_num=]Epoch 94:   4%|▍         | 22/546 [08:39<3:17:11, 22.58s/it, loss=0.0432, v_num=]Epoch 94:   4%|▍         | 23/546 [09:04<3:17:49, 22.70s/it, loss=0.0432, v_num=]Epoch 94:   4%|▍         | 23/546 [09:04<3:17:49, 22.70s/it, loss=0.0429, v_num=]Epoch 94:   4%|▍         | 24/546 [09:30<3:18:35, 22.83s/it, loss=0.0429, v_num=]Epoch 94:   4%|▍         | 24/546 [09:30<3:18:35, 22.83s/it, loss=0.0441, v_num=]Epoch 94:   5%|▍         | 25/546 [09:59<3:20:17, 23.07s/it, loss=0.0441, v_num=]Epoch 94:   5%|▍         | 25/546 [09:59<3:20:17, 23.07s/it, loss=0.0448, v_num=]Epoch 94:   5%|▍         | 26/546 [10:24<3:20:32, 23.14s/it, loss=0.0448, v_num=]Epoch 94:   5%|▍         | 26/546 [10:24<3:20:32, 23.14s/it, loss=0.0455, v_num=]Epoch 94:   5%|▍         | 27/546 [10:45<3:19:31, 23.07s/it, loss=0.0455, v_num=]Epoch 94:   5%|▍         | 27/546 [10:45<3:19:31, 23.07s/it, loss=0.0445, v_num=]Epoch 94:   5%|▌         | 28/546 [11:14<3:20:44, 23.25s/it, loss=0.0445, v_num=]Epoch 94:   5%|▌         | 28/546 [11:14<3:20:44, 23.25s/it, loss=0.0441, v_num=]Epoch 94:   5%|▌         | 29/546 [11:35<3:19:51, 23.19s/it, loss=0.0441, v_num=]Epoch 94:   5%|▌         | 29/546 [11:35<3:19:51, 23.19s/it, loss=0.0438, v_num=]Epoch 94:   5%|▌         | 30/546 [12:06<3:21:40, 23.45s/it, loss=0.0438, v_num=]Epoch 94:   5%|▌         | 30/546 [12:06<3:21:40, 23.45s/it, loss=0.0433, v_num=]Epoch 94:   6%|▌         | 31/546 [12:33<3:21:59, 23.53s/it, loss=0.0433, v_num=]Epoch 94:   6%|▌         | 31/546 [12:33<3:21:59, 23.53s/it, loss=0.0424, v_num=]Epoch 94:   6%|▌         | 32/546 [12:57<3:21:45, 23.55s/it, loss=0.0424, v_num=]Epoch 94:   6%|▌         | 32/546 [12:57<3:21:45, 23.55s/it, loss=0.044, v_num=] Epoch 94:   6%|▌         | 33/546 [13:23<3:22:01, 23.63s/it, loss=0.044, v_num=]Epoch 94:   6%|▌         | 33/546 [13:23<3:22:01, 23.63s/it, loss=0.0436, v_num=]